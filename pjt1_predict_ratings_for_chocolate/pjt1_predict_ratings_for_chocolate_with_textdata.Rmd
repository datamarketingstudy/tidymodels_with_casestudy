---
title: "Predict ratings for chocolate with tidymodels"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
```


*This template offers an opinionated guide on how to structure a modeling analysis. Your individual modeling analysis may require you to add to, subtract from, or otherwise change this structure, but consider this a general framework to start from. If you want to learn more about using tidymodels, check out our [Getting Started](https://www.tidymodels.org/start/) guide.*

### 준비 단계

```{r}
library(tidyverse)

url <- "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"
chocolate <- read_csv(url)
```
## 목표  

**tidymodels** 패키지를 활용하여 초콜릿 평가 점수(1점~5점 척도)에 대해 텍스트(text) 데이터를 활용하여 예측하고자 함.  
본 문서는 tidymodels 패키지에서 제공하는 r markdown 템플릿 **Model Aanalysis**를 활용하여 진행.  


## Explore data

Exploratory data analysis (EDA) is an [important part of the modeling process](https://www.tmwr.org/software-modeling.html#model-phases).  
1단계는 데이터에 대한 탐색 단계이며, 모델링의 목표는 각 초콜릿에 대한 특성(텍스트)을 기반으로 초콜릿의 ratings를 예측하고자 함.  
  
**Datasset**

```{r}
chocolate %>%
    head()
```
  
**초콜렛에 대한 ratings 분포를 파악**

```{r}
chocolate %>%
  ggplot(aes(rating)) +
  geom_histogram(bins = 15, fill = "midnightblue", alpha = 0.7)
```

**각 초콜렛에 대한 주요 특성(텍스트) 평가에서 가장 일반적으로 사용된 단어는 무엇일까?**  

```{r}
library(tidytext)

tidy_chocolate <- chocolate %>%
  unnest_tokens(word, most_memorable_characteristics)

tidy_chocolate %>% 
  count(word, sort = T)
```

**높은 평점을 받은 초콜렛과 낮은 평점을 받은 초콜렛에서 언급된 단어 그리고 평균 점수를 파악하기 위한 plot 생성**  

```{r}
tidy_chocolate %>%
  group_by(word) %>% 
  summarise(n = n(),
            rating = mean(rating)) %>% 
  ggplot(aes(n, rating)) +
  geom_hline(yintercept = mean(chocolate$rating), ## 평균 점수 나타내기
             lty = 2, color = "gray50", size = 1.2) +
  geom_point(color = "midnightblue", alpha = 0.7) +
  geom_text(aes(label = word),
            check_overlap = T, vjust = "top", hjust = "left") + ## 단어 나타내기
  scale_x_log10() ## 로그 스케일로 변환하여 분포 자세히 보기
```


## Build models

Let's consider how to [spend our data budget](https://www.tmwr.org/splitting.html):

- create training and testing sets
- create resampling folds from the *training* set

**기존 문서에서 실습하는 데이터셋 객체명으로 변경하여 사용할 수 있음**  


```{r}
library(tidymodels)

set.seed(123)
chocolate_split <- initial_split(chocolate, strata = rating)
choco_train <- training(chocolate_split)
choco_test <- testing(chocolate_split)

set.seed(234)
choco_folds <- vfold_cv(choco_train, strata = rating)
choco_folds
```

Let's set up our preprocessing :  

**텍스트 데이터를 토큰화하고, 사용이 적은 토큰은 제외, tf 적용**  


```{r}
# choco_train %>% select(rating, most_memorable_characteristics)

library(textrecipes) ## 텍스트 데이터 전처리를 위해 추가 패키지 라이브러리 

choco_rec <- recipe(rating ~ most_memorable_characteristics, data = choco_train) %>% 
  step_tokenize(most_memorable_characteristics) %>% 
  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% 
  step_tf(most_memorable_characteristics)
prep(choco_rec)
prep(choco_rec) %>% bake(new_data = NULL) %>% skimr::skim()
```



Let's create a [**model specification**](https://www.tmwr.org/models.html) for each model we want to try:
  
**우리가 다루는 예측 문제는 분류가 아닌 연속형 변수에 대한 예측이므로 regression으로 설정**  
비교할 두 가지 모델을 생성함. 1번 모델은 **랜덤 포레스트**, 2번 모델은 **SVM(서포트 벡터 머신)**을 사용하기로 함  
**랜덤 포레스트**의 경우 자연어 예측 시 잘 작동하는 것으로 알려져 있으며, **서포트 벡터 머신**은 텍스트 데이터와 잘 작동하는 경향이 있음

```{r}
ranger_spec <-
  rand_forest(trees = 500) %>% ## model 종류 선택
  set_engine("ranger") %>% ## 앤잔 선택
  set_mode("regression") ## 모델링의 문제 (classification or regression)
ranger_spec

svm_spec <- 
    svm_linear() %>%
    set_engine("LiblineaR") %>%
    set_mode("regression")
svm_spec
```

To set up your modeling code, consider using the [parsnip addin](https://parsnip.tidymodels.org/reference/parsnip_addin.html) or the [usemodels](https://usemodels.tidymodels.org/) package.

Now let's build a [**model workflow**](https://www.tmwr.org/workflows.html) combining each model specification with a data preprocessor:

```{r}
ranger_wf <- workflow(choco_rec, ranger_spec)
svm_wf <- workflow(choco_rec, svm_spec)
```

If your feature engineering needs are more complex than provided by a formula like `sex ~ .`, use a [recipe](https://www.tidymodels.org/start/recipes/). [Read more about feature engineering with recipes](https://www.tmwr.org/recipes.html) to learn how they work.


## Evaluate models

These models have no tuning parameters so we can evaluate them as they are. [Learn about tuning hyperparameters here.](https://www.tidymodels.org/start/tuning/)

```{r}
doParallel::registerDoParallel()
contrl_preds <- control_resamples(save_pred = TRUE)

svm_rs <- fit_resamples(
  svm_wf,
  resamples = choco_folds,
  control = contrl_preds
)

ranger_rs <- fit_resamples(
  ranger_wf,
  resamples = choco_folds,
  control = contrl_preds
)
```

How did these two models compare?

```{r}
collect_metrics(svm_rs)
collect_metrics(ranger_rs)
```

We can visualize these results :

```{r}
bind_rows(
  collect_predictions(svm_rs) %>%
    mutate(mod = "SVM"),
  collect_predictions(ranger_rs) %>%
    mutate(mod = "ranger")
) %>%
  ggplot(aes(rating, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray50", size = 1.2) +
  geom_jitter(width = 0.5, alpha = 0.5) +
  facet_wrap(vars(mod)) +
  coord_fixed()
```

These models perform very similarly, so perhaps we would choose the simpler, linear model. The function `last_fit()` *fits* one final time on the training data and *evaluates* on the testing data. This is the first time we have used the testing data.

```{r}
final_fitted <- last_fit(svm_wf, chocolate_split)
collect_metrics(final_fitted)  ## metrics evaluated on the *testing* data
```

This object contains a fitted workflow that we can use for prediction.

```{r}
final_wf <- extract_workflow(final_fitted)
predict(final_wf, choco_test[55,])
```

You can save this fitted `final_wf` object to use later with new data, for example with `readr::write_rds()`.

**최종적으로 어떤 단어가 높은 평가와 낮은 평가에 더 관련이 있는지 시각화하여 파악할 수 있음**  

```{r}
extract_workflow(final_fitted) %>%
  tidy() %>% 
  filter(term != "Bias") %>%
  group_by(estimate > 0) %>%
  slice_max(abs(estimate), n = 10) %>%
  ungroup() %>%
  mutate(term = str_remove(term, "tf_most_memorable_characteristics_")) %>%
  ggplot(aes(estimate, fct_reorder(term, estimate), fill = estimate > 0)) +
  geom_col(alpha = 0.8)
```

